{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "\n",
    "## Mashable news stories analysis\n",
    "\n",
    "Predicting if a news story is going to be popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2014/12/10/cia-torture-rep...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.487500</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/10/18/bitlock-kicksta...</td>\n",
       "      <td>447.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.135340</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/07/24/google-glass-po...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/11/21/these-are-the-m...</td>\n",
       "      <td>413.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677350</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.195701</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2014/02/11/parking-ticket-...</td>\n",
       "      <td>331.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2014/12/10/cia-torture-rep...       28.0   \n",
       "1  http://mashable.com/2013/10/18/bitlock-kicksta...      447.0   \n",
       "2  http://mashable.com/2013/07/24/google-glass-po...      533.0   \n",
       "3  http://mashable.com/2013/11/21/these-are-the-m...      413.0   \n",
       "4  http://mashable.com/2014/02/11/parking-ticket-...      331.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0             9.0             188.0         0.732620               1.0   \n",
       "1             7.0             297.0         0.653199               1.0   \n",
       "2            11.0             181.0         0.660377               1.0   \n",
       "3            12.0             781.0         0.497409               1.0   \n",
       "4             8.0             177.0         0.685714               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs   ...     \\\n",
       "0                  0.844262        5.0             1.0       1.0   ...      \n",
       "1                  0.815789        9.0             4.0       1.0   ...      \n",
       "2                  0.775701        4.0             3.0       1.0   ...      \n",
       "3                  0.677350       10.0             3.0       1.0   ...      \n",
       "4                  0.830357        3.0             2.0       1.0   ...      \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.200000                   0.80              -0.487500   \n",
       "1               0.160000                   0.50              -0.135340   \n",
       "2               0.136364                   1.00               0.000000   \n",
       "3               0.100000                   1.00              -0.195701   \n",
       "4               0.100000                   0.55              -0.175000   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.60              -0.250000                 0.9   \n",
       "1                  -0.40              -0.050000                 0.1   \n",
       "2                   0.00               0.000000                 0.3   \n",
       "3                  -0.40              -0.071429                 0.0   \n",
       "4                  -0.25              -0.100000                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.8                     0.4   \n",
       "1                      -0.1                     0.4   \n",
       "2                       1.0                     0.2   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  Popular  \n",
       "0                           0.8        1  \n",
       "1                           0.1        0  \n",
       "2                           1.0        0  \n",
       "3                           0.0        0  \n",
       "4                           0.0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/mashable.csv'\n",
    "train_df = pd.read_csv(url, index_col=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 61)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['url', 'Popular'], axis=1)\n",
    "y = train_df['Popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.1\n",
    "\n",
    "Estimate a Decision Tree Classifier and a Logistic Regresion\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(1)\n",
    "models = {'lr': LogisticRegressionCV(),\n",
    "          'dt': DecisionTreeClassifier()}\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_train, y_train)\n",
    "# predict test for each model\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr\n",
      "F1 Score:  0.5965156794425087\n",
      "Accurancy:  0.614\n",
      "dt\n",
      "F1 Score:  0.5402144772117962\n",
      "Accurancy:  0.5426666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "for model in models.keys():\n",
    "    print(model)\n",
    "    print('F1 Score: ', f1_score(y_pred[model], y_test))\n",
    "    print('Accurancy: ', metrics.accuracy_score(y_pred[model], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5673035636529471"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_pred.mean(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por F1-score y el Acc se evidencia que es mejor el modelo de Logit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.2\n",
    "\n",
    "Estimate 300 bagged samples\n",
    "\n",
    "Estimate the following set of classifiers:\n",
    "\n",
    "* 100 Decision Trees where max_depth=None\n",
    "* 100 Decision Trees where max_depth=2\n",
    "* 100 Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>dt_1</th>\n",
       "      <th>dt_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      log  dt_1  dt_2\n",
       "1483    1     1     1\n",
       "2185    1     1     1\n",
       "2520    1     1     1\n",
       "3721    1     1     1\n",
       "3727    0     0     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "models = {'log': LogisticRegression(),\n",
    "          'dt_1': DecisionTreeClassifier(max_depth=None),\n",
    "          'dt_2': DecisionTreeClassifier(max_depth=2)}\n",
    "y_pred = pd.DataFrame (columns =models.keys(),index= y_test.index )\n",
    "for i, model  in models.items():\n",
    "    bagreg = BaggingClassifier(model, n_estimators=100,bootstrap=True, oob_score=True, random_state=1,n_jobs=-1)\n",
    "    bagreg.fit(X_train, y_train)\n",
    "    y_pred[i] = bagreg.predict(X_test)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3\n",
    "\n",
    "Ensemble using majority voting\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log\n",
      "F1 Score:  0.6015352407536636\n",
      "Accurancy:  0.6193333333333333\n",
      "dt_1\n",
      "F1 Score:  0.635752688172043\n",
      "Accurancy:  0.6386666666666667\n",
      "dt_2\n",
      "F1 Score:  0.6434782608695653\n",
      "Accurancy:  0.6446666666666667\n",
      "Mean: \n",
      "f1 score:  0.6365475387727579 Acc:  0.6406666666666667\n"
     ]
    }
   ],
   "source": [
    "for model in models.keys():\n",
    "    print(model)\n",
    "    print('F1 Score: ', f1_score(y_pred[model], y_test))\n",
    "    print('Accurancy: ', metrics.accuracy_score(y_pred[model], y_test))\n",
    "print('Mean: ')\n",
    "print('f1 score: ',metrics.f1_score(np.round(y_pred.mean(axis=1)).astype(int), y_test), 'Acc: ',metrics.accuracy_score(np.round(y_pred.mean(axis=1)).astype(int), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.4\n",
    "\n",
    "Estimate te probability as %models that predict positive\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "th=[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\n",
    "models = {'log': LogisticRegression(),\n",
    "          'dt_1': DecisionTreeClassifier(max_depth=None),\n",
    "          'dt_2': DecisionTreeClassifier(max_depth=2)}\n",
    "f1_t = []\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for i in th:\n",
    "    for model in models.keys():\n",
    "        bagreg = BaggingClassifier(models[model], n_estimators=100,bootstrap=True, oob_score=True, random_state=1,n_jobs=-1)\n",
    "        bagreg.fit(X_train, y_train)\n",
    "        y_pred[model] = bagreg.predict_proba(X_test)\n",
    "        y_pred[model] = y_pred[model].apply(lambda x: 1 if x>=i else 0)\n",
    "        f1_t.append([i,f1_score(y_test,y_pred[model]),metrics.accuracy_score(y_pred[model], y_test),model])\n",
    "f1_t=pd.DataFrame(f1_t, columns=['Threshold','F1 Score','Acc','Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score in log:\n",
      "Threshold         0.1\n",
      "F1 Score     0.667557\n",
      "Acc             0.502\n",
      "Model             log\n",
      "Name: 27, dtype: object\n",
      "\n",
      "Best F1 Score in dt_1:\n",
      "Threshold         0.1\n",
      "F1 Score     0.667851\n",
      "Acc          0.501333\n",
      "Model            dt_1\n",
      "Name: 28, dtype: object\n",
      "\n",
      "Best F1 Score in dt_2:\n",
      "Threshold         0.2\n",
      "F1 Score     0.670213\n",
      "Acc             0.504\n",
      "Model            dt_2\n",
      "Name: 26, dtype: object\n"
     ]
    }
   ],
   "source": [
    "flog = f1_t[f1_t.Model=='log']\n",
    "fdt1 = f1_t[f1_t.Model=='dt_1']\n",
    "fdt2 = f1_t[f1_t.Model=='dt_2']\n",
    "print('Best F1 Score in log:')\n",
    "print(flog.loc[flog['F1 Score'].idxmax()])\n",
    "print()\n",
    "print('Best F1 Score in dt_1:')\n",
    "print(fdt1.loc[fdt1['F1 Score'].idxmax()])\n",
    "print()\n",
    "print('Best F1 Score in dt_2:')\n",
    "print(fdt2.loc[fdt2['F1 Score'].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin modificar el Threshold pasa lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 Score:\n",
      "Threshold         0.5\n",
      "F1 Score     0.411653\n",
      "Acc          0.380667\n",
      "Model             log\n",
      "Name: 0, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "th=[0.5]\n",
    "models = {'log': LogisticRegression(),\n",
    "          'dt_1': DecisionTreeClassifier(max_depth=None),\n",
    "          'dt_2': DecisionTreeClassifier(max_depth=2)}\n",
    "f1_t = []\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for i in th:\n",
    "    for model in models.keys():\n",
    "        bagreg = BaggingClassifier(models[model], n_estimators=100,bootstrap=True, oob_score=True, random_state=1,n_jobs=-1)\n",
    "        bagreg.fit(X_train, y_train)\n",
    "        y_pred[model] = bagreg.predict_proba(X_test)\n",
    "        y_pred[model] = y_pred[model].apply(lambda x: 1 if x>=i else 0)\n",
    "        f1_t.append([i,f1_score(y_test,y_pred[model]),metrics.accuracy_score(y_pred[model], y_test),model])\n",
    "f1_t=pd.DataFrame(f1_t, columns=['Threshold','F1 Score','Acc','Model'])\n",
    "print()\n",
    "print('Best F1 Score:')\n",
    "print(f1_t.loc[f1_t['F1 Score'].idxmax()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.5\n",
    "\n",
    "Ensemble using weighted voting using the oob_error\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'log': LogisticRegression(),\n",
    "          'dt_1': DecisionTreeClassifier(max_depth=None),\n",
    "          'dt_2': DecisionTreeClassifier(max_depth=2)}\n",
    "f1_t = []\n",
    "mod2={}\n",
    "for model in models.keys():\n",
    "    mod2[model] = BaggingClassifier(models[model], n_estimators=100,bootstrap=True, oob_score=True, random_state=1,n_jobs=-1)\n",
    "    mod2[model].fit(X_train, y_train)\n",
    "    y_pred[model] = mod2[model].predict(X_test)\n",
    "    f1_t.append([metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test),model])\n",
    "f1_t=pd.DataFrame(f1_t, columns=['F1 Score','Acc','Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=[]\n",
    "for model in mod2.keys():\n",
    "    errors = np.zeros(mod2[model].n_estimators)\n",
    "    y_pred_all_ = np.zeros((X_test.shape[0], mod2[model].n_estimators))\n",
    "    for i in range(mod2[model].n_estimators):\n",
    "        oob_sample = ~mod2[model].estimators_samples_[i]\n",
    "        y_pred_ = mod2[model].estimators_[i].predict(X_train.values[oob_sample])\n",
    "        errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "        y_pred_all_[:, i] = mod2[model].estimators_[i].predict(X_test)\n",
    "    alpha = (1 - errors) / (1 - errors).sum()\n",
    "    y_pred[model] = (np.sum(y_pred_all_ * alpha, axis=1) >= 0.5).astype(np.int)\n",
    "    f1.append([metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test),model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 Score:\n",
      "F1 Score    0.643955\n",
      "Acc         0.644667\n",
      "Model           dt_2\n",
      "Name: 2, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=pd.DataFrame(f1, columns=['F1 Score','Acc','Model'])\n",
    "print()\n",
    "print('Best F1 Score:')\n",
    "print(f1.loc[f1['F1 Score'].idxmax()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.6\n",
    "\n",
    "Estimate te probability of the weighted voting\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 Score:\n",
      "Threshold         0.3\n",
      "F1 Score     0.697148\n",
      "Acc          0.589333\n",
      "Model            dt_1\n",
      "Name: 22, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f2=[]\n",
    "th=[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\n",
    "for j in th:\n",
    "    for model in mod2.keys():\n",
    "        errors = np.zeros(mod2[model].n_estimators)\n",
    "        y_pred_all_ = np.zeros((X_test.shape[0], mod2[model].n_estimators))\n",
    "        for i in range(mod2[model].n_estimators):\n",
    "            oob_sample = ~mod2[model].estimators_samples_[i]\n",
    "            y_pred_ = mod2[model].estimators_[i].predict(X_train.values[oob_sample])\n",
    "            errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "            y_pred_all_[:, i] = mod2[model].estimators_[i].predict(X_test)\n",
    "        alpha = (1 - errors) / (1 - errors).sum()\n",
    "        y_pred[model] = (np.sum(y_pred_all_ * alpha, axis=1) >= j).astype(np.int)\n",
    "        f2.append([j, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test),model])\n",
    "f2=pd.DataFrame(f2, columns=['Threshold','F1 Score','Acc','Model'])\n",
    "print()\n",
    "print('Best F1 Score:')\n",
    "print(f2.loc[f2['F1 Score'].idxmax()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.7\n",
    "\n",
    "Estimate a logistic regression using as input the estimated classifiers\n",
    "\n",
    "Modify the probability threshold such that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 Score:\n",
      "Model           dt_2\n",
      "F1 Score    0.634048\n",
      "Acc            0.636\n",
      "Name: 2, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff=[]\n",
    "for model in mod2.keys():\n",
    "    y_pred_all_ = np.zeros((X_test.shape[0], mod2[model].n_estimators))\n",
    "    X_train_3 = np.zeros((X_train.shape[0], mod2[model].n_estimators))\n",
    "    for i in range(mod2[model].n_estimators):\n",
    "        X_train_3[:, i] = mod2[model].estimators_[i].predict(X_train)\n",
    "        y_pred_all_[:, i] = mod2[model].estimators_[i].predict(X_test)\n",
    "    lr = LogisticRegressionCV(cv=5)\n",
    "    lr.fit(X_train_3, y_train)\n",
    "    y_pred[model] = lr.predict(y_pred_all_)\n",
    "    ff.append([model, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test)])\n",
    "ff=pd.DataFrame(ff, columns=['Model','F1 Score','Acc'])\n",
    "print()\n",
    "print('Best F1 Score:')\n",
    "print(ff.loc[ff['F1 Score'].idxmax()])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
