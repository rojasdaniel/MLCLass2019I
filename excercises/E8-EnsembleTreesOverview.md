El uso común de machine learning por sobre otras metodologías de predicción es su alta precisión producto de aprender previamente los patrones de un set de datos bajo un algoritmo que, posiblemente, mejor representa las características propias de la información (dependiendo de si lo que quiero es predecir variables continuas o clasificar binariamente, etc). Algunos modelos suelen tener mejor performance que otros lo que lleva a preguntarse: ¿y si pudiese obtener los mejores estimadores para predecir una variable en función de dos o más modelos entrenados previamente? Es como modelar lo mejor de cada modelo (valga la redundancia), en un único modelo que, para garantizar gobernanza entre tantos modelos, se plantea el uso de estrategias tales como: Majority Voting o Weighted Voting o Stacking.
Los métodos de ensamblaje son precisamente los encargados de encontrar la mejor forma de utilizar más de 2 modelos en uno único que parte de la hipótesis de que al ser modelos totalmente independientes y bajo metodologías de cálculo diferentes; capturan diferentes patrones en función de sus características matemáticas y una vez se obtienen los modelos por aparte, es posible combinarlos para generar una precisión inclusive mayor a los modelos separados.

Estos métodos de ensamblaje al promediar los scores de predicción de varios modelos, es muy útil para mejorar los índices de predicción, pero cuentan con una desventaja que, en algunas circunstancias, puede ser relevante para el problema que se esté trabajando; la perdida de interpretabilidad de los estimadores es completa lo que dificulta adquirir insights sobre los features que se utilizan para predecir en los modelos y adicionalmente es computacionalmente mucho más pesado dado que requiere mayor poder de computo para identificar un único modelo producto de varios (es por eso que siempre se recomienda utilizar modelos de predicción relativamente rápidos como arboles de decisión para disminuir el tiempo de computación)
